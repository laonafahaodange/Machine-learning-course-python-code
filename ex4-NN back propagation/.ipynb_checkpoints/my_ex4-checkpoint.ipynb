{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f1b43e",
   "metadata": {},
   "source": [
    "## 使用反向传播的前馈神经网络进行手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c207e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6748654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loadmat('ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2aff78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data['X']\n",
    "y=data['y']\n",
    "\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df770776",
   "metadata": {},
   "source": [
    "对标签y进行one-hot编码。one-hot 编码将类标签n（k类）转换为长度为k的向量，其中索引n为“hot”（1），而其余为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878da63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scikitlearn内置one-hot编码\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder(sparse=False)#参数sparse分析见下\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f58f4",
   "metadata": {},
   "source": [
    "初始化OneHotEncoder实例时，默认sparse参数为True，编码后返回的是一个稀疏矩阵的对象，如果要使用一般要调用toarray()方法转化成array对象。若将sparse参数设置为False，则直接生成array对象，可直接使用。  \n",
    "矩阵的绝大部分数值为零，且非零元素呈不规律分布时，则称该矩阵为稀疏矩阵（Sparse Matrix）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66584db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检查编码\n",
    "y[0], y_onehot[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01057b9",
   "metadata": {},
   "source": [
    "神经网络结构分为三层，输入层->隐藏层->输出层。  \n",
    "根据特征数可得输入层为400个输入单元+1个偏置单元；隐藏层这里设置为25个单元+1个偏置单元；输出层根据one-hot编码为10个单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a899fa",
   "metadata": {},
   "source": [
    "# sigmoid 函数\n",
    "g 代表一个常用的逻辑函数（logistic function）为S形函数（Sigmoid function），公式为： \\\\[g\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}\\\\] \n",
    "合起来，我们得到逻辑回归模型的假设函数： \n",
    "\t\\\\[{{h}_{\\theta }}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{\\theta }^{T}}X}}}\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ffa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义sigmoid函数\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47614daa",
   "metadata": {},
   "source": [
    "## 神经网络结构  \n",
    "<img style=\"float: left;\" src=\"../img/nn_model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e948d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前向传播\n",
    "def forward_propagate(X,theta1,theta2):\n",
    "    m=X.shape[0]#样本数\n",
    "    \n",
    "    a1=np.insert(X,0,values=np.ones(m),axis=1)#输入层的偏置项5000 x 401\n",
    "    z2=a1*theta1.T\n",
    "    \n",
    "    a2=sigmoid(z2)\n",
    "    a2=np.insert(a2,0,values=np.ones(m),axis=1)#隐藏层的偏置项\n",
    "    \n",
    "    z3=a2*theta2.T\n",
    "    a3=sigmoid(z3)#输出\n",
    "    \n",
    "    return a1,a2,a3,z2,z3#a3也是h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd729b",
   "metadata": {},
   "source": [
    "# 代价函数\n",
    "<img style=\"float: left;\" src=\"../img/nn_cost.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e482d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(params,input_size,hidden_size,num_labels,X,y,learning_rate):\n",
    "    m=X.shape[0]\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    \n",
    "    #获取权重\n",
    "    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))\n",
    "    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))\n",
    "    \n",
    "    #前向传播\n",
    "    a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #计算代价函数\n",
    "    J=0\n",
    "    for i in range(m):\n",
    "        #尝试由下面两行代码读懂公式的计算方式\n",
    "        #注意这里的y是经过one-hot编码的，维度不是m x 1\n",
    "        first_term=np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n",
    "        \n",
    "        sum_term=np.sum(first_term-second_term)\n",
    "        J=J+sum_term\n",
    "    \n",
    "    J=J/m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdebd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化设置\n",
    "input_size=400\n",
    "hidden_size=25\n",
    "num_labels=10\n",
    "learning_rate=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4fa34ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04475686,  0.01297965, -0.10706175, ..., -0.11446212,\n",
       "        -0.03227179,  0.03240322]),\n",
       " (10285,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机初始化完整网络参数大小的参数数组\n",
    "#np.random.random返回随机的浮点数，在半开区间 [0.0, 1.0)\n",
    "#根据表达式可知初始化参数值在[-0.5/4,0.5/4)之间\n",
    "params=(np.random.random(size=hidden_size*(input_size+1)+num_labels*(hidden_size+1))-0.5)*0.25\n",
    "params,params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1da39b",
   "metadata": {},
   "source": [
    "关于参数的数目:  \n",
    "由神经网络结构的输入层->隐藏层可知有__(输入层单元数+1)x隐藏层__个参数  \n",
    "由隐藏层->输出层可知有__(隐藏层+1)x输出层__个参数  \n",
    "所以总参数数量为__(输入层单元数+1)x隐藏层+(隐藏层+1)x输入层__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6272cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将params拆开成输入层->隐藏层和隐藏层->输出层的参数（权重）矩阵\n",
    "theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))\n",
    "theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))\n",
    "theta1.shape,theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b300f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 26), (5000, 10), (5000, 25), (5000, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "a1.shape,a2.shape,h.shape,z2.shape,z3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc724ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.184086601814853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用代价函数计算本次前向传播的h和y的误差\n",
    "#由于初始化params的值每次都不一样，所以cost的结果会和参考代码不同\n",
    "cost(params,input_size,hidden_size,num_labels,X,y_onehot,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a4d0f",
   "metadata": {},
   "source": [
    "# 正则化代价函数\n",
    "下一步是代价函数的正则化\n",
    "<img style=\"float: left;\" src=\"../img/nn_regcost.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b18450e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costReg(params,input_size,hidden_size,num_labels,X,y,learning_rate):\n",
    "    #前面部分和cost是一样的\n",
    "    m=X.shape[0]\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    \n",
    "    #获取权重\n",
    "    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))\n",
    "    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))\n",
    "    \n",
    "    #前向传播\n",
    "    a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #计算代价函数\n",
    "    J=0\n",
    "    for i in range(m):\n",
    "        #尝试由下面两行代码读懂公式的计算方式\n",
    "        #注意这里的y是经过one-hot编码的，维度不是m x 1\n",
    "        first_term=np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n",
    "        \n",
    "        sum_term=np.sum(first_term-second_term)\n",
    "        J=J+sum_term\n",
    "    J=J/m\n",
    "    \n",
    "    #正则化部分\n",
    "    #theta1 25 x 401\n",
    "    #theta2 10 x 26\n",
    "    first_term=np.sum(np.power(theta1[:,1:],2))#索引从1开始是因为连接偏置单元的权重无需正则化,下同\n",
    "    second_term=np.sum(np.power(theta2[:,1:],2))\n",
    "    reg=learning_rate/(2*m)*(first_term+second_term)\n",
    "    \n",
    "    J_reg=J+reg\n",
    "    \n",
    "    return J_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e08d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.189391087876465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costReg(params,input_size,hidden_size,num_labels,X,y_onehot,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d004a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算sigmoid函数梯度的函数，这是计算误差的其中一步\n",
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z),(1-sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b71c8",
   "metadata": {},
   "source": [
    "## 再来回顾下神经网络结构  \n",
    "<img style=\"float: left;\" src=\"../img/nn_model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "207d3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#反向传播函数用于计算梯度\n",
    "def backPropagate(params,input_size,hidden_size,num_labels,X,y,learning_rate):\n",
    "    m=X.shape[0]\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    \n",
    "    #获取权重\n",
    "    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))\n",
    "    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))\n",
    "    \n",
    "    #前向传播\n",
    "    a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #初始化\n",
    "    J=0\n",
    "    delta1=np.zeros_like(theta1)#(25, 401)\n",
    "    delta2=np.zeros_like(theta2)#(10, 26)\n",
    "    \n",
    "    #计算代价函数\n",
    "    for i in range(m):\n",
    "        first_term=np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n",
    "        sum_term=np.sum(first_term-second_term)\n",
    "        J=J+sum_term\n",
    "    J=J/m\n",
    "    \n",
    "    #正则化部分\n",
    "    first_term=np.sum(np.power(theta1[:,1:],2))\n",
    "    second_term=np.sum(np.power(theta2[:,1:],2))\n",
    "    reg=learning_rate/(2*m)*(first_term+second_term)\n",
    "    \n",
    "    J_reg=J+reg\n",
    "    \n",
    "    #反向传播，类比参考笔记中的解析（笔记中的网络共四层，本练习为三层）\n",
    "    for i in range(m):\n",
    "        a1i=a1[i,:]# 1 x 401\n",
    "        z2i=z2[i,:]# 1 x 25\n",
    "        a2i=a2[i,:]# 1 x 26\n",
    "        hi=h[i,:]# 1 x 10\n",
    "        yi=y[i,:]# 1 x 10\n",
    "        \n",
    "        d3i=hi-yi# 1 x 10\n",
    "        \n",
    "        z2i=np.insert(z2i,0,np.ones(1))# 1 x 26\n",
    "        d2i=np.multiply(d3i*theta2,sigmoid_gradient(z2i))#1 x 26\n",
    "        \n",
    "        delta2=delta2+d3i.T*a2i# 10 x 26\n",
    "        delta1=delta1+d2i[:,1:].T*a1i# 25 x 401，由于z2i多插入了1列，所以索引要从1开始才能对应维度\n",
    "        \n",
    "    #不考虑正则化\n",
    "    delta1=delta1/m\n",
    "    delta2=delta2/m\n",
    "    \n",
    "    #把两个偏导数矩阵拼接成一维矩阵\n",
    "    grad=np.concatenate((np.ravel(delta1),np.ravel(delta2)))\n",
    "    \n",
    "    return J_reg,grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647245e",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"../img/back_propagate.png\">  \n",
    "<img style=\"float: left;\" src=\"../img/back_propagate1.jpg\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b0774",
   "metadata": {},
   "source": [
    "再说明一下：  \n",
    "A * B与np.multiply（A，B）使用。 基本上前者是矩阵乘法，后者是元素乘法，除非A或B是标量值（常数？），在这种情况下没关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7f3b35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.189391087876465, (10285,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J,grad=backPropagate(params,input_size,hidden_size,num_labels,X,y_onehot,learning_rate)\n",
    "J,grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fae73a",
   "metadata": {},
   "source": [
    "为反向传播添加正则化  \n",
    "<img style=\"float: left;\" src=\"../img/back_propagate1.png\"> \n",
    "笔记里这个公式有误，应该为  \n",
    "<img style=\"float: left;\" src=\"../img/back_propagate2.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97676f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#反向传播函数用于计算梯度\n",
    "def backPropagateReg(params,input_size,hidden_size,num_labels,X,y,learning_rate):\n",
    "    m=X.shape[0]\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    \n",
    "    #获取权重\n",
    "    theta1=np.matrix(np.reshape(params[:hidden_size*(input_size+1)],(hidden_size,(input_size+1))))\n",
    "    theta2=np.matrix(np.reshape(params[hidden_size*(input_size+1):],(num_labels,(hidden_size+1))))\n",
    "    \n",
    "    #前向传播\n",
    "    a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "    \n",
    "    #初始化\n",
    "    J=0\n",
    "    delta1=np.zeros_like(theta1)#(25, 401)\n",
    "    delta2=np.zeros_like(theta2)#(10, 26)\n",
    "    \n",
    "    #计算代价函数\n",
    "    for i in range(m):\n",
    "        first_term=np.multiply(-y[i,:],np.log(h[i,:]))\n",
    "        second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n",
    "        sum_term=np.sum(first_term-second_term)\n",
    "        J=J+sum_term\n",
    "    J=J/m\n",
    "    \n",
    "    #正则化部分\n",
    "    first_term=np.sum(np.power(theta1[:,1:],2))\n",
    "    second_term=np.sum(np.power(theta2[:,1:],2))\n",
    "    reg=learning_rate/(2*m)*(first_term+second_term)\n",
    "    \n",
    "    J_reg=J+reg\n",
    "    \n",
    "    #反向传播，类比参考笔记中的解析（笔记中的网络共四层，本练习为三层）\n",
    "    for i in range(m):\n",
    "        a1i=a1[i,:]# 1 x 401\n",
    "        z2i=z2[i,:]# 1 x 25\n",
    "        a2i=a2[i,:]# 1 x 26\n",
    "        hi=h[i,:]# 1 x 10\n",
    "        yi=y[i,:]# 1 x 10\n",
    "        \n",
    "        d3i=hi-yi# 1 x 10\n",
    "        \n",
    "        z2i=np.insert(z2i,0,np.ones(1))# 1 x 26\n",
    "        d2i=np.multiply(d3i*theta2,sigmoid_gradient(z2i))#1 x 26\n",
    "        \n",
    "        delta2=delta2+d3i.T*a2i# 10 x 26\n",
    "        delta1=delta1+d2i[:,1:].T*a1i# 25 x 401，由于z2i多插入了1列，所以索引要从1开始才能对应维度\n",
    "        \n",
    "    #考虑正则化\n",
    "    delta1=delta1/m\n",
    "    delta2=delta2/m\n",
    "    delta1[:,1:]=delta1[:,1:]+learning_rate*theta1[:,1:]/m\n",
    "    delta2[:,1:]=delta2[:,1:]+learning_rate*theta2[:,1:]/m\n",
    "    \n",
    "    #把两个偏导数矩阵拼接成一维矩阵\n",
    "    grad=np.concatenate((np.ravel(delta1),np.ravel(delta2)))\n",
    "    \n",
    "    return J_reg,grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c4726ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.189391087876465, (10285,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J,grad=backPropagateReg(params,input_size,hidden_size,num_labels,X,y_onehot,learning_rate)\n",
    "J,grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df03df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-4a377a3d9d98>:22: RuntimeWarning: divide by zero encountered in log\n",
      "  second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n",
      "<ipython-input-36-4a377a3d9d98>:22: RuntimeWarning: invalid value encountered in multiply\n",
      "  second_term=np.multiply((1-y[i,:]),np.log(1-h[i,:]))\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "#求最小化代价函数对应的参数\n",
    "#fun：该参数就是costFunction，要去最小化的损失函数，损失函数在定义时，theta必须为第一个参数且其shape必须为(n,)即一维数组\n",
    "#x0：初始化的theta,其shape必须为shape(n,)即一维数组\n",
    "#args：其他参数，如X，Y，lambda等\n",
    "#method：该参数代表采用的方式，默认是BFGS, L-BFGS-B, SLSQP中的一种，可选TNC\n",
    "#jac：该参数就是计算梯度的函数，和fun参数类似，第一个必须为theta且其shape必须为(n,)即一维数组,最后返回的梯度也必须为一个一维数组。\n",
    "#     如果jac是布尔值且为真，则假定fun返回objective和gradient作为（f，g）元组\n",
    "#options：迭代次数\n",
    "fmin=minimize(fun=backPropagateReg,x0=params,args=(input_size,hidden_size,num_labels,X,y_onehot,learning_rate),method='TNC',jac=True,options={'maxiter':250})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07fb5384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     fun: 0.3978926454720155\n",
       "      jac: array([ 6.58121657e-04,  1.59280059e-06, -1.18515336e-05, ...,\n",
       "        -1.93584670e-04, -4.61652035e-07, -4.35948163e-04])\n",
       "  message: 'Linear search failed'\n",
       "     nfev: 235\n",
       "      nit: 15\n",
       "   status: 4\n",
       "  success: False\n",
       "        x: array([ 0.27907172,  0.007964  , -0.05925767, ..., -1.13192774,\n",
       "        -1.58880955, -0.96514877]),\n",
       " (10285,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin,fmin.x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f54cae",
   "metadata": {},
   "source": [
    "反向传播计算梯度的同时也计算了损失函数，调用scipy中optimize的minimize求出了对应的权重矩阵（迭代250次的条件下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee7d002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.61999999999999%\n"
     ]
    }
   ],
   "source": [
    "#利用一次前向传播测试准确率\n",
    "X=np.matrix(X)\n",
    "theta1=np.reshape(fmin.x[:hidden_size*(input_size+1)],(hidden_size,(input_size+1)))\n",
    "theta2=np.reshape(fmin.x[hidden_size*(input_size+1):],(num_labels,(hidden_size+1)))\n",
    "\n",
    "a1,a2,h,z2,z3=forward_propagate(X,theta1,theta2)\n",
    "#h是5000 x 10 维的数组\n",
    "h_argmax=np.argmax(h,axis=1)\n",
    "h_argmax=h_argmax+1#标签从1开始，0对应标签10\n",
    "\n",
    "#h_argmax是矩阵对象（5000 x 1），需要转成array对象\n",
    "y_predict=np.array(h_argmax)\n",
    "\n",
    "#统计准确率\n",
    "correct=0\n",
    "for i in range(len(y_predict)):\n",
    "    if y_predict[i]==data['y'][i]:\n",
    "        correct=correct+1\n",
    "\n",
    "accuracy=correct/(len(y_predict))\n",
    "print('accuracy = {0}%'.format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
